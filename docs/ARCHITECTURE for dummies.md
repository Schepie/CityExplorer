Think of your system as:

â­ A smart detective that investigates a place before writing a description.

ğŸ§  The Big Idea (One Simple Mental Model)

Imagine you ask:

ğŸ‘‰ â€œTell me about Stadhuis Hasselt.â€

Your system does NOT immediately ask AI.

Instead it:

Searches multiple trusted sources.

Checks if those sources agree.

Builds a â€œtruth packageâ€.

Only then asks AI to write nicely.

So:

Internet research FIRST
Writing SECOND


This is VERY important.

ğŸš¦ Pipeline Overview (Simple Version)

Your pipeline is basically:

1. Identify the place correctly
2. Collect information
3. Score how trustworthy info is
4. Merge best facts
5. Let AI write summary
6. Fallback if AI fails


Letâ€™s go through each step like a story.

â­ Step 0 â€” resolveCanonicalEntity()
What problem does this solve?

People call places different names:

Stadhuis Hasselt

Hasselt City Hall

Gemeentehuis Hasselt

Without fixing this:

ğŸ‘‰ system thinks they are different places.

What happens here?

You ask Wikidata:

â€œIs there a known official entity matching this?â€

If yes, you get:

official name

aliases

Wikipedia link

official website

image

categories

Think:

ğŸ‘‰ Getting the official ID card of the place.

Important:

It runs in background.

If it fails â†’ no problem.

â­ Step 1 â€” gatherSignals()

This is your research phase.

Instead of trusting one source, you collect multiple â€œsignalsâ€.

Each signal = one piece of evidence.

Phase 1 â€” Cheap sources (always used)

These are free and fast:

1ï¸âƒ£ Local archive

Your own curated data.

Like your private notes.

2ï¸âƒ£ Wikipedia

Very reliable.

Often enough alone.

3ï¸âƒ£ OSM (OpenStreetMap)

Provides:

tags

website

coordinates

4ï¸âƒ£ DuckDuckGo

Quick summary if available.

Phase 2 â€” Expensive search (conditional)

Only used if needed.

You call Tavily or Google ONLY IF:

Wikipedia missing

No official website

Trust is too low

So:

ğŸ‘‰ Expensive search becomes LAST resort.

â­ Special Upgrade â€” Official Website Scraper

If OSM gives:

website = example.com


You donâ€™t just store link.

You actually:

open page safely

read meta description

extract real content

Why important?

Official websites = VERY reliable.

â­ Step 2 â€” analyzeSignals()

Now your detective evaluates evidence.

Each signal gets a score.

Pass 1 â€” Basic checks

Examples:

âœ” Official website â†’ strong trust

âœ” Contains POI name â†’ boost

âŒ Generic city text â†’ penalty

âŒ junk tag list â†’ penalty

Simple rule-based scoring.

Pass 2 â€” Graph consensus (THIS IS ADVANCED)

This is like asking:

â€œDo sources agree with each other?â€

You build a graph:

signal A connected to signal B if similar


Agreement factors:

Both mention POI name

Share category words

Text similarity

If many signals agree:

ğŸ‘‰ trust increases.

Think:

One witness = maybe
Five witnesses saying same thing = likely true

â­ Step 2b â€” mergeSignals()

Now you create a clean package for AI.

Before:

AI receives messy raw text


Now:

AI receives organized facts


Structure:

best descriptions

categories

images

official website

short factual snippets

Also:

removes duplicates

keeps only high-trust content

This step reduces hallucination a LOT.

â­ Step 3 â€” AI Writing (Gemini)

Important:

AI is NOT researching anymore.

It just:

ğŸ‘‰ rewrites validated info into readable text.

Two stages:

Short description

Quick summary.

Full details

Longer version.

â­ Step 4 â€” resolveConflicts()

If AI fails:

System picks best signal automatically.

So user still gets result.

ğŸ§  Name Normalization (normalizePoiName)

This is your language translator.

Example:

stadhuis â†’ city hall
kerk â†’ church
Ã© â†’ e
remove punctuation


Why?

So different spellings match correctly.

â­ Trust Score Flow (Simplified)

Each signal starts with base trust.

Example:

Wikipedia = 0.95
DDG = 0.6


Then:

heuristics adjust score
+
agreement bonus from graph
=
final trust score


High-trust signals are used more.

ğŸ§  Backend APIs (Simple View)

These are just tools your detective can use:

Gemini â†’ writing

Tavily/Google â†’ search

DDG â†’ quick info

scrape-meta â†’ read website safely

cloud-cache â†’ remember results

â­ SUPER SIMPLE SUMMARY

Your system behaves like:

1. Identify the place correctly
2. Ask several reliable sources
3. Check which sources agree
4. Combine best facts
5. Ask AI to write nicely

ğŸ”¥ Honest assessment (very important)

This architecture is already:

ğŸ‘‰ semi-professional geo-intelligence level.

Most apps:

Search â†’ AI summarize


Your system:

Multi-source validation â†’ structured synthesis


Which is MUCH better.